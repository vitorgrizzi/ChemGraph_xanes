{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Overview","text":"<p>ChemGraph</p> <p>ChemGraph is an agentic framework that can automate molecular simulation workflows using large language models (LLMs). Built on top of <code>LangGraph</code> and <code>ASE</code>, ChemGraph allows users to perform complex computational chemistry tasks, from structure generation to thermochemistry calculations, with a natural language interface. </p> <p>ChemGraph</p> <p>ChemGraph supports diverse simulation backends, including ab initio quantum chemistry methods (e.g. coupled-cluster, DFT via NWChem, ORCA), semi-empirical methods (e.g., XTB via TBLite), and machine learning potentials (e.g, MACE, UMA) through a modular integration with <code>ASE</code>.</p> <p>Docker Image</p> <p>ChemGraph Docker images are published to GHCR at <code>ghcr.io/argonne-lcf/chemgraph</code>. See Docker Support for build, run, and publishing details.</p>"},{"location":"acknowledgements/","title":"Acknowledgements","text":"<p>Info</p> <p>This research used resources of the Argonne Leadership Computing Facility, a U.S. Department of Energy (DOE) Office of Science user facility at Argonne National Laboratory and is based on research supported by the U.S. DOE Office of Science- Advanced Scientific Computing Research Program, under Contract No. DE-AC02- 06CH11357. Our work leverages ALCF Inference Endpoints, which provide a robust API for LLM inference on ALCF HPC clusters via Globus Compute. We are thankful to Serkan Altunta\u015f for his contributions to the user interface of ChemGraph and for insightful discussions on AIOps.</p>"},{"location":"citation/","title":"Citation","text":"<p>If you use ChemGraph in your research, please cite our work:</p> <pre><code>```bibtex\n@article{pham2025chemgraph,\ntitle={ChemGraph: An Agentic Framework for Computational Chemistry Workflows},\nauthor={Pham, Thang D and Tanikanti, Aditya and Ke\u00e7eli, Murat},\njournal={arXiv preprint arXiv:2506.06363},\nyear={2025}\nurl={https://arxiv.org/abs/2506.06363}\n}\n```\n</code></pre>"},{"location":"code_formatting_and_linting/","title":"Code Formatting & Linting","text":"<p>This project uses Ruff for both formatting and linting. To ensure all code follows our style guidelines, install the pre-commit hook:</p> <pre><code>pip install pre-commit\npre-commit install\n</code></pre>"},{"location":"configuration_with_toml/","title":"Configuration with TOML","text":"<p>Note</p> <p>ChemGraph supports comprehensive configuration through TOML files, allowing you to customize model settings, API configurations, chemistry parameters, and more.</p>"},{"location":"configuration_with_toml/#configuration-file-structure","title":"Configuration File Structure","text":"<p>Create a <code>config.toml</code> file in your project directory to configure ChemGraph behavior:</p> <pre><code># ChemGraph Configuration File\n# This file contains all configuration settings for ChemGraph CLI and agents\n\n[general]\n# Default model to use for queries\nmodel = \"gpt-4o-mini\"\n# Workflow type: single_agent, multi_agent, python_repl, graspa\nworkflow = \"single_agent\"\n# Output format: state, last_message\noutput = \"state\"\n# Enable structured output\nstructured = false\n# Generate detailed reports\nreport = true\n\n# Recursion limit for agent workflows\nrecursion_limit = 20\n# Enable verbose output\nverbose = false\n\n[llm]\n# Temperature for LLM responses (0.0 to 1.0)\ntemperature = 0.1\n# Maximum tokens for responses\nmax_tokens = 4000\n# Top-p sampling parameter\ntop_p = 0.95\n# Frequency penalty (-2.0 to 2.0)\nfrequency_penalty = 0.0\n# Presence penalty (-2.0 to 2.0)\npresence_penalty = 0.0\n\n[api]\n# Custom base URLs for different providers\n[api.openai]\nbase_url = \"https://api.openai.com/v1\"\ntimeout = 30\n\n[api.anthropic]\nbase_url = \"https://api.anthropic.com\"\ntimeout = 30\n\n[api.google]\nbase_url = \"https://generativelanguage.googleapis.com/v1beta\"\ntimeout = 30\n\n[api.local]\n# For local models like Ollama\nbase_url = \"http://localhost:11434\"\ntimeout = 60\n\n[chemistry]\n# Default calculation settings\n[chemistry.optimization]\n# Optimization method: BFGS, L-BFGS-B, CG, etc.\nmethod = \"BFGS\"\n# Force tolerance for convergence\nfmax = 0.05\n# Maximum optimization steps\nsteps = 200\n\n[chemistry.frequencies]\n# Displacement for finite difference\ndisplacement = 0.01\n# Number of processes for parallel calculation\nnprocs = 1\n\n[chemistry.calculators]\n# Default calculator for different tasks\ndefault = \"mace_mp\"\n# Available calculators: mace_mp, emt, nwchem, orca, psi4, tblite\nfallback = \"emt\"\n\n[output]\n# Output file settings\n[output.files]\n# Default output directory\ndirectory = \"./chemgraph_output\"\n# File naming pattern\npattern = \"{timestamp}_{query_hash}\"\n# Supported formats: xyz, json, html, png\nformats = [\"xyz\", \"json\", \"html\"]\n\n[output.visualization]\n# 3D visualization settings\nenable_3d = true\n# Molecular viewer: py3dmol, ase_gui\nviewer = \"py3dmol\"\n# Image resolution for saved figures\ndpi = 300\n\n[logging]\n# Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL\nlevel = \"INFO\"\n# Log file location\nfile = \"./chemgraph.log\"\n# Enable console logging\nconsole = true\n\n[features]\n# Enable experimental features\nenable_experimental = false\n# Enable caching of results\nenable_cache = true\n# Cache directory\ncache_dir = \"./cache\"\n# Cache expiration time in hours\ncache_expiry = 24\n\n[security]\n# Enable API key validation\nvalidate_keys = true\n# Enable request rate limiting\nrate_limit = true\n# Max requests per minute\nmax_requests_per_minute = 60\n</code></pre>"},{"location":"configuration_with_toml/#using-configuration-files","title":"Using Configuration Files","text":""},{"location":"configuration_with_toml/#with-the-command-line-interface","title":"With the Command Line Interface","text":"<pre><code># Use configuration file\nchemgraph --config config.toml -q \"What is the SMILES string for water?\"\n\n# Override specific settings\nchemgraph --config config.toml -q \"Optimize methane\" -m gpt-4o --verbose\n</code></pre>"},{"location":"configuration_with_toml/#argoopenai-compatible-endpoints","title":"Argo/OpenAI-Compatible Endpoints","text":"<p>For Argo or any OpenAI-compatible endpoint, set <code>api.openai.base_url</code> in <code>config.toml</code>. Optional <code>api.openai.argo_user</code> can also be configured.</p> <pre><code>[api.openai]\nbase_url = \"https://apps-dev.inside.anl.gov/argoapi/v1\"\nargo_user = \"your_argo_username\"\n</code></pre> <p><code>ARGO_USER</code> is only used as a fallback when <code>argo_user</code> is not provided in <code>config.toml</code>.</p>"},{"location":"configuration_with_toml/#configuration-sections","title":"Configuration Sections","text":"Section Description <code>[general]</code> Basic settings like model, workflow, and output format <code>[llm]</code> LLM-specific parameters (temperature, max_tokens, etc.) <code>[api]</code> API endpoints and timeouts for different providers <code>[chemistry]</code> Chemistry-specific calculation settings <code>[output]</code> Output file formats and visualization settings <code>[logging]</code> Logging configuration and verbosity levels <code>[features]</code> Feature flags and experimental settings <code>[security]</code> Security settings and rate limiting"},{"location":"configuration_with_toml/#command-line-interface","title":"Command Line Interface","text":"<p>ChemGraph includes a powerful command-line interface (CLI) that provides all the functionality of the web interface through the terminal. The CLI features rich formatting, interactive mode, and comprehensive configuration options.</p>"},{"location":"configuration_with_toml/#installation-setup","title":"Installation &amp; Setup","text":"<p>The CLI is included by default when you install ChemGraph:</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"configuration_with_toml/#basic-usage","title":"Basic Usage","text":""},{"location":"configuration_with_toml/#quick-start","title":"Quick Start","text":"<pre><code># Basic query\nchemgraph -q \"What is the SMILES string for water?\"\n\n# With model selection\nchemgraph -q \"Optimize methane geometry\" -m gpt-4o\n\n# With report generation\nchemgraph -q \"Calculate CO2 vibrational frequencies\" -r\n\n# Using configuration file\nchemgraph --config config.toml -q \"Your query here\"\n</code></pre>"},{"location":"configuration_with_toml/#command-syntax","title":"Command Syntax","text":"<pre><code>chemgraph [OPTIONS] -q \"YOUR_QUERY\"\n</code></pre>"},{"location":"configuration_with_toml/#command-line-options","title":"Command Line Options","text":"<p>Core Arguments:</p> Option Short Description Default <code>--query</code> <code>-q</code> The computational chemistry query to execute Required <code>--model</code> <code>-m</code> LLM model to use <code>gpt-4o-mini</code> <code>--workflow</code> <code>-w</code> Workflow type <code>single_agent</code> <code>--output</code> <code>-o</code> Output format (<code>state</code>, <code>last_message</code>) <code>state</code> <code>--structured</code> <code>-s</code> Use structured output format <code>False</code> <code>--report</code> <code>-r</code> Generate detailed report <code>False</code> <p>Model Selection:</p> <pre><code># OpenAI models\nchemgraph -q \"Your query\" -m gpt-4o\nchemgraph -q \"Your query\" -m gpt-4o-mini\nchemgraph -q \"Your query\" -m o1-preview\n\n# Anthropic models\nchemgraph -q \"Your query\" -m claude-3-5-sonnet-20241022\nchemgraph -q \"Your query\" -m claude-3-opus-20240229\n\n# Google models\nchemgraph -q \"Your query\" -m gemini-1.5-pro\n\n# Local models (OpenAI-compatible local endpoint)\nchemgraph -q \"Your query\" -m llama-3.1-70b-instruct\n</code></pre> <p>Workflow Types:</p> <pre><code># Single agent (default) - best for most tasks\nchemgraph -q \"Optimize water molecule\" -w single_agent\n\n# Multi-agent - complex tasks with planning\nchemgraph -q \"Complex analysis\" -w multi_agent\n\n# Python REPL - interactive coding\nchemgraph -q \"Write analysis code\" -w python_repl\n\n# gRASPA - molecular simulation\nchemgraph -q \"Run adsorption simulation\" -w graspa\n</code></pre> <p>Output Formats:</p> <pre><code># Full state output (default)\nchemgraph -q \"Your query\" -o state\n\n# Last message only\nchemgraph -q \"Your query\" -o last_message\n\n# Structured output\nchemgraph -q \"Your query\" -s\n\n# Generate detailed report\nchemgraph -q \"Your query\" -r\n</code></pre>"},{"location":"configuration_with_toml/#interactive-mode","title":"Interactive Mode","text":"<p>Start an interactive session for continuous conversations:</p> <pre><code>chemgraph --interactive\n</code></pre> <p>Interactive Features: - Persistent conversation: Maintain context across queries - Model switching: Change models mid-conversation - Workflow switching: Switch between different agent types - Built-in commands: Help, clear, config, etc.</p> <p>Interactive Commands: <pre><code># In interactive mode, type:\nhelp                    # Show available commands\nclear                   # Clear screen\nconfig                  # Show current configuration\nquit                    # Exit interactive mode\nmodel gpt-4o           # Change model\nworkflow multi_agent   # Change workflow\n</code></pre></p>"},{"location":"configuration_with_toml/#utility-commands","title":"Utility Commands","text":"<p>List Available Models: <pre><code>chemgraph --list-models\n</code></pre></p> <p>Check API Keys: <pre><code>chemgraph --check-keys\n</code></pre></p> <p>Get Help: <pre><code>chemgraph --help\n</code></pre></p>"},{"location":"configuration_with_toml/#configuration-file-support","title":"Configuration File Support","text":"<p>Use TOML configuration files for consistent settings:</p> <pre><code>chemgraph --config config.toml -q \"Your query\"\n</code></pre>"},{"location":"configuration_with_toml/#environment-variables","title":"Environment Variables","text":"<p>Provider keys and optional endpoint settings are read from environment variables and <code>config.toml</code> (for example, <code>api.openai.base_url</code> and <code>api.openai.argo_user</code>).</p>"},{"location":"configuration_with_toml/#advanced-options","title":"Advanced Options","text":"<p>Timeout and Error Handling: <pre><code># Set recursion limit\nchemgraph -q \"Complex query\" --recursion-limit 30\n\n# Verbose output for debugging\nchemgraph -q \"Your query\" -v\n\n# Save output to file\nchemgraph -q \"Your query\" --output-file results.txt\n</code></pre></p>"},{"location":"configuration_with_toml/#example-workflows","title":"Example Workflows","text":"<p>Basic Molecular Analysis: <pre><code># Get molecular structure\nchemgraph -q \"What is the SMILES string for caffeine?\"\n\n# Optimize geometry\nchemgraph -q \"Optimize the geometry of caffeine using DFT\" -m gpt-4o -r\n\n# Calculate properties\nchemgraph -q \"Calculate the vibrational frequencies of optimized caffeine\" -r\n</code></pre></p> <p>Interactive Research Session: <pre><code># Start interactive mode\nchemgraph --interactive\n\n# Select model and workflow\n&gt; model gpt-4o\n&gt; workflow single_agent\n\n# Conduct analysis\n&gt; What is the structure of aspirin?\n&gt; Optimize its geometry using DFT\n&gt; Calculate its electronic properties\n&gt; Compare with ibuprofen\n</code></pre></p> <p>Batch Processing: <pre><code># Process multiple queries\nchemgraph -q \"Analyze water molecule\" --output-file water_analysis.txt\nchemgraph -q \"Analyze methane molecule\" --output-file methane_analysis.txt\nchemgraph -q \"Analyze ammonia molecule\" --output-file ammonia_analysis.txt\n</code></pre></p>"},{"location":"configuration_with_toml/#api-key-setup","title":"API Key Setup","text":"<p>Required API Keys: <pre><code># OpenAI (for GPT models)\nexport OPENAI_API_KEY=\"your_openai_key_here\"\n\n# Anthropic (for Claude models)\nexport ANTHROPIC_API_KEY=\"your_anthropic_key_here\"\n\n# Google (for Gemini models)\nexport GEMINI_API_KEY=\"your_gemini_key_here\"\n</code></pre></p> <p>Getting API Keys: - OpenAI: Visit platform.openai.com/api-keys - Anthropic: Visit console.anthropic.com - Google: Visit aistudio.google.com/apikey</p>"},{"location":"configuration_with_toml/#performance-tips","title":"Performance Tips","text":"<ul> <li>Use <code>gpt-4o-mini</code> for faster, cost-effective queries</li> <li>Use <code>gpt-4o</code> for complex analysis requiring higher reasoning</li> <li>Enable <code>--report</code> for detailed documentation</li> <li>Use <code>--structured</code> output for programmatic parsing</li> <li>Leverage configuration files for consistent settings</li> </ul>"},{"location":"configuration_with_toml/#troubleshooting","title":"Troubleshooting","text":"<p>Common Issues: <pre><code># Check API key status\nchemgraph --check-keys\n\n# Verify model availability\nchemgraph --list-models\n\n# Test with verbose output\nchemgraph -q \"test query\" -v\n\n# Check configuration\nchemgraph --config config.toml -q \"test\" --verbose\n</code></pre></p> <p>Error Messages: - \"Invalid model\": Use <code>--list-models</code> to see available options - \"API key not found\": Use <code>--check-keys</code> to verify setup - \"Query required\": Use <code>-q</code> to specify your query - \"Timeout\": Increase <code>--recursion-limit</code> or simplify query</p> <p>The CLI provides: - Beautiful terminal output with colors and formatting powered by Rich - API key validation before agent initialization - Timeout protection to prevent hanging processes - Interactive mode for continuous conversations - Configuration file support with TOML format - Environment-specific settings for development/production - Comprehensive help and examples for all features</p>"},{"location":"docker_support/","title":"Docker Support","text":"<p>Note</p> <p>Docker setup is now intentionally simplified and does not include vLLM. The same ChemGraph image can be launched in four modes: JupyterLab, Streamlit UI, MCP server, or interactive CLI.</p>"},{"location":"docker_support/#prerequisites","title":"Prerequisites","text":"<ul> <li>Docker</li> <li>Docker Compose</li> </ul>"},{"location":"docker_support/#files","title":"Files","text":"<ul> <li><code>Dockerfile</code>: standard ChemGraph container image</li> <li><code>Dockerfile.arm</code>: ARM-friendly variant (same runtime goals, no vLLM)</li> <li><code>docker-compose.yml</code>: profile-based launcher for Jupyter/Streamlit/MCP</li> </ul>"},{"location":"docker_support/#use-published-ghcr-image-no-local-build","title":"Use Published GHCR Image (No Local Build)","text":"<p>If you do not want a local install, run the published container image directly:</p>"},{"location":"docker_support/#pass-api-keys-securely-best-practice","title":"Pass API Keys Securely (Best Practice)","text":"<p>Required keys depend on the model/provider you use:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GEMINI_API_KEY</code></li> <li><code>GROQ_API_KEY</code></li> <li>Optional: <code>ARGO_USER</code> (for Argo setups)</li> </ul> <p>Recommended pattern for <code>docker run</code> is host pass-through (do not put secret values inline in command history):</p> <pre><code>export OPENAI_API_KEY=\"...\"\ndocker run --rm -it -e OPENAI_API_KEY -p 8501:8501 ghcr.io/argonne-lcf/chemgraph:latest \\\n  streamlit run src/ui/app.py --server.address=0.0.0.0 --server.port=8501\n</code></pre> <p>For multiple keys, use an env file:</p> <pre><code>cat &gt; .env.chemgraph &lt;&lt; 'EOF'\nOPENAI_API_KEY=...\nANTHROPIC_API_KEY=...\nGEMINI_API_KEY=...\nGROQ_API_KEY=...\nARGO_USER=...\nEOF\nchmod 600 .env.chemgraph\ndocker run --rm -it --env-file .env.chemgraph -p 8501:8501 ghcr.io/argonne-lcf/chemgraph:latest \\\n  streamlit run src/ui/app.py --server.address=0.0.0.0 --server.port=8501\n</code></pre> <p>Security notes:</p> <ul> <li>Pass only the key(s) needed for your selected model.</li> <li>Do not commit <code>.env.chemgraph</code> or other secret files to git.</li> <li>Avoid storing API keys in <code>config.toml</code>.</li> </ul> <p>Run JupyterLab:</p> <pre><code>docker run --rm -it -p 8888:8888 ghcr.io/argonne-lcf/chemgraph:latest \\\n  jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --LabApp.token=\n</code></pre> <p>Run Streamlit:</p> <pre><code>docker run --rm -it -p 8501:8501 ghcr.io/argonne-lcf/chemgraph:latest \\\n  streamlit run src/ui/app.py --server.address=0.0.0.0 --server.port=8501\n</code></pre> <p>Run MCP server (HTTP):</p> <pre><code>docker run --rm -it -p 9003:9003 ghcr.io/argonne-lcf/chemgraph:latest \\\n  python -m chemgraph.mcp.mcp_tools --transport streamable_http --host 0.0.0.0 --port 9003\n</code></pre> <p>Run interactive CLI shell:</p> <pre><code>docker run --rm -it --entrypoint /bin/bash -v \"$PWD:/work\" -w /work \\\n  ghcr.io/argonne-lcf/chemgraph:latest\n</code></pre> <p>Then inside the container:</p> <pre><code>chemgraph --help\nchemgraph --config config.toml -q \"calculate the energy for smiles=O using mace_mp\"\n</code></pre>"},{"location":"docker_support/#build-image","title":"Build Image","text":"<p>From project root:</p> <pre><code>docker compose build\n</code></pre> <p>If you previously built the image before this update, rebuild so the Git safe-directory setting for <code>/app</code> is included.</p>"},{"location":"docker_support/#run-jupyterlab","title":"Run JupyterLab","text":"<pre><code>docker compose --profile jupyter up\n</code></pre> <p>Access: <code>http://localhost:8888</code></p>"},{"location":"docker_support/#run-streamlit-app","title":"Run Streamlit App","text":"<pre><code>docker compose --profile streamlit up\n</code></pre> <p>Access: <code>http://localhost:8501</code></p>"},{"location":"docker_support/#run-mcp-server-http-transport","title":"Run MCP Server (HTTP transport)","text":"<pre><code>docker compose --profile mcp up\n</code></pre> <p>MCP endpoint: <code>http://localhost:9003</code></p> <p>The compose service launches:</p> <pre><code>python -m chemgraph.mcp.mcp_tools --transport streamable_http --host 0.0.0.0 --port 9003\n</code></pre>"},{"location":"docker_support/#environment-variables","title":"Environment Variables","text":"<p>The compose file forwards these variables into the container when set on host:</p> <ul> <li><code>OPENAI_API_KEY</code></li> <li><code>ANTHROPIC_API_KEY</code></li> <li><code>GEMINI_API_KEY</code></li> <li><code>GROQ_API_KEY</code></li> <li><code>CHEMGRAPH_LOG_DIR</code> (default in compose: <code>/app/cg_logs</code>)</li> <li><code>PYTHONPATH</code> is set to <code>/app/src</code> in compose so bind-mounted source code is used.</li> </ul> <p>Example:</p> <pre><code>export OPENAI_API_KEY=\"your_key\"\ndocker compose --profile streamlit up\n</code></pre>"},{"location":"docker_support/#stop-services","title":"Stop Services","text":"<pre><code>docker compose down\n</code></pre>"},{"location":"docker_support/#run-without-compose-optional","title":"Run Without Compose (Optional)","text":"<p>Build once:</p> <pre><code>docker build -t chemgraph:local .\n</code></pre> <p>Jupyter:</p> <pre><code>docker run --rm -it -p 8888:8888 -v \"$PWD:/app\" chemgraph:local \\\n  jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --LabApp.token=\n</code></pre> <p>Streamlit:</p> <pre><code>docker run --rm -it -p 8501:8501 -v \"$PWD:/app\" chemgraph:local \\\n  streamlit run src/ui/app.py --server.address=0.0.0.0 --server.port=8501\n</code></pre> <p>MCP (HTTP):</p> <pre><code>docker run --rm -it -p 9003:9003 -v \"$PWD:/app\" chemgraph:local \\\n  python -m chemgraph.mcp.mcp_tools --transport streamable_http --host 0.0.0.0 --port 9003\n</code></pre>"},{"location":"docker_support/#notes","title":"Notes","text":"<ul> <li>This setup avoids local model-serving dependencies and keeps Docker usage focused on ChemGraph tooling.</li> <li>If you need local LLM serving, run it as a separate service outside this Docker setup and point ChemGraph to that endpoint via model/base URL configuration.</li> <li>Compose startup also runs <code>git config --global --add safe.directory /app</code> to avoid   Git \"dubious ownership\" errors in notebooks/Streamlit when the repo is bind-mounted.</li> <li>The default <code>Dockerfile</code> installs <code>nwchem</code> and <code>tblite</code> with conda-forge.</li> </ul>"},{"location":"docker_support/#publish-to-ghcr","title":"Publish to GHCR","text":"<p>A GitHub Actions workflow (<code>.github/workflows/ghcr-publish.yml</code>) publishes the Docker image to:</p> <ul> <li><code>ghcr.io/&lt;org-or-user&gt;/chemgraph:&lt;tag&gt;</code></li> <li><code>ghcr.io/&lt;org-or-user&gt;/chemgraph:sha-&lt;commit&gt;</code></li> </ul> <p>How to publish:</p> <pre><code>git tag v0.3.0\ngit push origin v0.3.0\n</code></pre> <p>You can also trigger the workflow manually from Actions with <code>workflow_dispatch</code>.</p>"},{"location":"example_usage/","title":"Example Usage","text":"<p>Note</p> <p>Before exploring example usage in the <code>notebooks/</code> directory, ensure you have specified the necessary API tokens in your environment. </p> OpenAI API KeyAnthropic API KeyGoogle AI Studio (Gemini) API Key <ol> <li> <p>Log in to your OpenAI account at the OpenAI Platform website. If you don't have an account, you'll need to create one first.</p> </li> <li> <p>Navigate to the API keys section. You can find this by clicking on your profile icon in the top-right corner and selecting \"API keys.\"</p> </li> <li> <p>Click the + Create new secret key button.</p> </li> <li> <p>Give your key a descriptive name (e.g., \"ChemGraph\").</p> </li> <li> <p>Click Create secret key. A new key will be generated.</p> </li> <li> <p>Copy the key and save it in a secure location. You will not be able to see it again after this step.</p> </li> <li> <p>Set the key in your environment using the command provided in the instructions:      <pre><code>export OPENAI_API_KEY=\"your_api_key_here\"  # On Unix or macOS\nsetx OPENAI_API_KEY \"your_api_key_here\"  # On Windows\n</code></pre></p> </li> <li>Restart your terminal or IDE to ensure the environment variable is loaded.</li> </ol> <ol> <li> <p>Sign up or log in to your Anthropic account at the Anthropic console.</p> </li> <li> <p>In the left-hand navigation menu, select API Keys.</p> </li> <li> <p>Click on the option to create a new API key.</p> </li> <li> <p>Provide a name for your API key (e.g., \"ChemGraph\").</p> </li> <li> <p>Click Create Key again.</p> </li> <li> <p>Copy the generated key and store it securely, as you may not be able to view it again.</p> </li> <li> <p>Set the key in your environment using the command provided in the instructions:      <pre><code>export ANTHROPIC_API_KEY=\"your_api_key_here\"  # On Unix or macOS\nsetx ANTHROPIC_API_KEY \"your_api_key_here\"  # On Windows\n</code></pre></p> </li> <li>Restart your terminal or IDE to ensure the environment variable is loaded.</li> </ol> <ol> <li> <p>Go to the Google AI Studio website at Google AI Studio and sign in with your Google account.</p> </li> <li> <p>In the left-hand menu, select Get API key.</p> </li> <li> <p>Click the Create API key in new project button. A new key will be instantly generated.</p> </li> <li> <p>Copy the API key by clicking the copy icon next to it.</p> </li> <li> <p>Set the key as an environment variable:      <pre><code>export GEMINI_API_KEY=\"your_api_key_here\"  # On Unix or macOS\nsetx GEMINI_API_KEY \"your_api_key_here\"  # On Windows\n</code></pre></p> </li> <li>Restart your terminal or IDE to ensure the environment variable is loaded.</li> </ol> Explore Example Notebooks <p>Navigate to the <code>notebooks/</code> directory to explore various example notebooks demonstrating different capabilities of ChemGraph.</p> <ul> <li> <p>Single-Agent System with MACE: This notebook demonstrates how a single agent can utilize multiple tools with MACE/xTB support.</p> </li> <li> <p>Single-Agent System with UMA: This notebook demonstrates how a single agent can utilize multiple tools with UMA support.</p> </li> <li> <p>Multi-Agent System: This notebook demonstrates a multi-agent setup where different agents (Planner, Executor and Aggregator) handle various tasks exemplifying the collaborative potential of ChemGraph.</p> </li> <li> <p>Model Context Protocol (MCP) Server: This notebook shows how to run and connect to ChemGraph MCP tooling.</p> </li> <li> <p>Single-Agent System with gRASPA: This notebook provides a sample guide on executing a gRASPA simulation using a single agent. For gRASPA-related installation instructions, visit the gRASPA GitHub repository. The notebook's functionality has been validated on a single compute node at ALCF Polaris.</p> </li> </ul>"},{"location":"installation/","title":"Installation","text":"<p>Note</p> <p>ChemGraph requires Python 3.10+.</p>"},{"location":"installation/#install-from-pypi-recommended","title":"Install from PyPI (recommended)","text":"<pre><code>pip install chemgraphagent\n</code></pre> <p>Default installation does not require <code>tblite</code>.</p> <p>To include optional calculator extras (including <code>tblite</code>):</p> <pre><code>pip install \"chemgraphagent[calculators]\"\n</code></pre> <p>Warning</p> <p>On platforms without a prebuilt <code>tblite</code> wheel, installing <code>calculators</code> may require a local Fortran toolchain.</p>"},{"location":"installation/#install-from-source","title":"Install from source","text":""},{"location":"installation/#pipvenv","title":"pip/venv","text":"<pre><code>git clone https://github.com/argonne-lcf/ChemGraph\ncd ChemGraph\npython -m venv chemgraph-env\nsource chemgraph-env/bin/activate  # Windows: .\\chemgraph-env\\Scripts\\activate\npip install -e .\n</code></pre>"},{"location":"installation/#conda","title":"conda","text":"<pre><code>git clone --depth 1 https://github.com/argonne-lcf/ChemGraph\ncd ChemGraph\nconda env create -f environment.yml\nconda activate chemgraph\n</code></pre>"},{"location":"installation/#uv","title":"uv","text":"<pre><code>git clone https://github.com/argonne-lcf/ChemGraph\ncd ChemGraph\nuv venv --python 3.11 chemgraph-env\nsource chemgraph-env/bin/activate  # Windows: .\\chemgraph-env\\Scripts\\activate\nuv pip install -e .\n</code></pre>"},{"location":"installation/#optional-uma-install","title":"Optional UMA install","text":"<p><code>uma</code> and <code>mace-torch</code> can conflict through different <code>e3nn</code> requirements. Use separate environments if you need both MACE and UMA.</p> <p>PyPI attempt:</p> <pre><code>pip install \"chemgraphagent[uma]\"\n</code></pre> <p>From source:</p> <pre><code>pip install -e \".[uma]\"\n</code></pre> <p>If resolution fails, install UMA in a separate environment dedicated to UMA workflows.</p>"},{"location":"license/","title":"License","text":"<p>Info</p> <p>This project is licensed under the Apache 2.0 License.</p>"},{"location":"mcp_servers/","title":"MCP Servers","text":"<p>Note</p> <p>ChemGraph exposes tools through Model Context Protocol (MCP) servers in <code>src/chemgraph/mcp/</code>.</p>"},{"location":"mcp_servers/#available-servers","title":"Available servers","text":"<ul> <li><code>mcp_tools.py</code>: general ASE-powered chemistry tools</li> <li><code>mace_mcp_parsl.py</code>: MACE + Parsl workflows</li> <li><code>graspa_mcp_parsl.py</code>: gRASPA + Parsl workflows</li> <li><code>data_analysis_mcp.py</code>: analysis utilities for generated results</li> </ul>"},{"location":"mcp_servers/#run-a-server","title":"Run a server","text":""},{"location":"mcp_servers/#stdio-transport-default","title":"stdio transport (default)","text":"<pre><code>python -m chemgraph.mcp.mcp_tools\n</code></pre>"},{"location":"mcp_servers/#streamable-http-transport","title":"streamable HTTP transport","text":"<pre><code>python -m chemgraph.mcp.mcp_tools --transport streamable_http --host 0.0.0.0 --port 9003\n</code></pre>"},{"location":"mcp_servers/#common-cli-options","title":"Common CLI options","text":"<p>All MCP servers use:</p> <ul> <li><code>--transport</code> with <code>stdio</code> or <code>streamable_http</code></li> <li><code>--host</code> for HTTP mode</li> <li><code>--port</code> for HTTP mode</li> </ul>"},{"location":"mcp_servers/#docker-mode","title":"Docker mode","text":"<p>You can run MCP server mode with Docker Compose:</p> <pre><code>docker compose --profile mcp up\n</code></pre> <p>Endpoint: <code>http://localhost:9003</code></p>"},{"location":"mcp_servers/#notes-for-parsl-based-servers","title":"Notes for Parsl-based servers","text":"<p><code>mace_mcp_parsl.py</code> and <code>graspa_mcp_parsl.py</code> rely on Parsl and HPC-specific configuration. Ensure your environment is prepared for the target system before running production jobs.</p>"},{"location":"project_structure/","title":"Project Structure","text":"<pre><code>chemgraph/\n\u2502\n\u251c\u2500\u2500 src/                       # Source code\n\u2502   \u251c\u2500\u2500 chemgraph/             # Top-level package\n\u2502   \u2502   \u251c\u2500\u2500 agent/             # Agent-based task management\n\u2502   \u2502   \u251c\u2500\u2500 graphs/            # Workflow graph utilities\n\u2502   \u2502   \u251c\u2500\u2500 models/            # Different Pydantic models\n\u2502   \u2502   \u251c\u2500\u2500 mcp/               # MCP servers (stdio/streamable HTTP)\n\u2502   \u2502   \u251c\u2500\u2500 prompt/            # Agent prompt\n\u2502   \u2502   \u251c\u2500\u2500 state/             # Agent state\n\u2502   \u2502   \u251c\u2500\u2500 tools/             # Tools for molecular simulations\n\u2502   \u2502   \u251c\u2500\u2500 utils/             # Other utility functions\n\u2502   \u251c\u2500\u2500 ui/                    # CLI and Streamlit UI package\n\u2502\n\u251c\u2500\u2500 docs/                      # MkDocs documentation\n\u251c\u2500\u2500 pyproject.toml             # Project configuration\n\u2514\u2500\u2500 README.md                  # Project documentation\n</code></pre>"},{"location":"running_local_models/","title":"Running Local Models with vLLM","text":"<p>Note</p> <p>This section describes how to set up and run local language models using the vLLM inference server.</p>"},{"location":"running_local_models/#inference-backend-setup-remotelocal","title":"Inference Backend Setup (Remote/Local)","text":""},{"location":"running_local_models/#virtual-python-environment","title":"Virtual Python Environment","text":"<p>All instructions below must be executed within a Python virtual environment. Ensure the virtual environment uses the same Python version as your project (e.g., Python 3.11).</p> <p>Example 1: Using conda <pre><code>conda create -n vllm-env python=3.11 -y\nconda activate vllm-env\n</code></pre></p> <p>Example 2: Using python venv <pre><code>python3.11 -m venv vllm-env\nsource vllm-env/bin/activate  # On Windows use `vllm-env\\\\Scripts\\\\activate`\n</code></pre></p>"},{"location":"running_local_models/#install-inference-server-vllm","title":"Install Inference Server (vLLM)","text":"<p>vLLM is recommended for serving many transformer models efficiently.</p> <p>Basic vLLM installation from source: Make sure your virtual environment is activated. <pre><code># Ensure git is installed\ngit clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .\n</code></pre> For specific hardware acceleration (e.g., CUDA, ROCm), refer to the official vLLM installation documentation.</p>"},{"location":"running_local_models/#running-the-vllm-server-standalone","title":"Running the vLLM Server (Standalone)","text":"<p>A script is provided at <code>scripts/run_vllm_server.sh</code> to help start a vLLM server with features like logging, retry attempts, and timeout. This is intended for running vLLM as a separate standalone service (for example, on a machine with GPU access).</p> <p>Before running the script: 1.  Ensure your vLLM Python virtual environment is activated.     <pre><code># Example: if you used conda\n# conda activate vllm-env \n# Example: if you used python venv\n# source path/to/your/vllm-env/bin/activate\n</code></pre> 2.  Make the script executable:     <pre><code>chmod +x scripts/run_vllm_server.sh\n</code></pre></p> <p>To run the script:</p> <pre><code>./scripts/run_vllm_server.sh [MODEL_IDENTIFIER] [PORT] [MAX_MODEL_LENGTH]\n</code></pre> <ul> <li><code>[MODEL_IDENTIFIER]</code> (optional): The Hugging Face model identifier. Defaults to <code>facebook/opt-125m</code>.</li> <li><code>[PORT]</code> (optional): The port for the vLLM server. Defaults to <code>8001</code>.</li> <li><code>[MAX_MODEL_LENGTH]</code> (optional): The maximum model length. Defaults to <code>4096</code>.</li> </ul> <p>Example: <pre><code>./scripts/run_vllm_server.sh meta-llama/Meta-Llama-3-8B-Instruct 8001 8192\n</code></pre></p> Important Note on Gated Models (e.g., Llama 3): <ul> <li> <p>Many models, such as those from the Llama family by Meta, are gated and require you to accept their terms of use on Hugging Face and use an access token for download. </p> </li> <li> <p>To use such models with vLLM:</p> <ol> <li>Hugging Face Account and Token: Ensure you have a Hugging Face account and have generated an access token with <code>read</code> permissions. You can find this in your Hugging Face account settings under \"Access Tokens\".</li> <li>Accept Model License: Navigate to the Hugging Face page of the specific model you want to use (e.g., <code>meta-llama/Meta-Llama-3-8B-Instruct</code>) and accept its license/terms if prompted.</li> <li>Environment Variables: Before running the vLLM server, set the following environment variables in your terminal session or environment configuration (e.g., <code>.bashrc</code>, <code>.zshrc</code>):     <pre><code>export HF_TOKEN=\"your_hugging_face_token_here\"\n# Optional: Specify a directory for Hugging Face to download models and cache.\n# export HF_HOME=\"/path/to/your/huggingface_cache_directory\"\n</code></pre>     vLLM will use these environment variables to authenticate with Hugging Face and download the model weights.</li> </ol> </li> <li> <p>The script will:</p> <ul> <li>Attempt to start the vLLM OpenAI-compatible API server.</li> <li>Log output to a file in the <code>logs/</code> directory (created if it doesn't exist at the project root).</li> <li>The server runs in the background via <code>nohup</code>.</li> </ul> </li> <li> <p>This standalone script is the recommended approach for users who manage their own vLLM instances directly.</p> </li> </ul>"},{"location":"streamlit_web_interface/","title":"Streamlit Web Interface","text":"<p>Note</p> <p>ChemGraph includes a Streamlit web UI for chat-driven chemistry workflows, structure visualization, and report viewing.</p>"},{"location":"streamlit_web_interface/#run-the-app","title":"Run the app","text":"<p>Set provider keys as needed:</p> <pre><code>export OPENAI_API_KEY=\"...\"\nexport ANTHROPIC_API_KEY=\"...\"\nexport GEMINI_API_KEY=\"...\"\n</code></pre> <p>Launch:</p> <pre><code>streamlit run src/ui/app.py\n</code></pre> <p>Then open <code>http://localhost:8501</code>.</p>"},{"location":"streamlit_web_interface/#features","title":"Features","text":"<ul> <li>Chat interface for single-agent and multi-agent workflows</li> <li>Model selection across supported providers</li> <li>3D molecular visualization with <code>stmol</code>/<code>py3Dmol</code></li> <li>Embedded report display and structure export</li> <li>Config editor for <code>config.toml</code></li> </ul>"},{"location":"streamlit_web_interface/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>If 3D rendering is unavailable, install <code>stmol</code>:   <code>pip install stmol</code></li> <li>If model calls fail, verify API keys and endpoint settings in <code>config.toml</code>.</li> <li>If Argo is used, ensure <code>api.openai.base_url</code> and optional <code>api.openai.argo_user</code> are configured.</li> </ul>"}]}